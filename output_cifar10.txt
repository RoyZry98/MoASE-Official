# hidden dimension = 48
[24/05/13 22:34:01] [conf.py:  216]: PyTorch Version: torch=1.10.0, cuda=11.3, cudnn=8200
[24/05/13 22:34:01] [conf.py:  218]: BN:
  EPS: 1e-05
  MOM: 0.1
CKPT_DIR: ./ckpt
CORRUPTION:
  DATASET: cifar10
  NUM_EX: 10000
  SEVERITY: [5]
  TYPE: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
CUDNN:
  BENCHMARK: True
DATA_DIR: you/path/to/data
DESC: 
LOG_DEST: vit_240513_223401.txt
LOG_TIME: 240513_223401
MODEL:
  ADAPTATION: cotta
  ARCH: Standard_VITB
  EPISODIC: False
OPTIM:
  AP: 0.92
  BETA: 0.9
  DAMPENING: 0.0
  LR: 0.0001
  METHOD: Adam
  MOMENTUM: 0.9
  MT: 0.999
  NESTEROV: True
  RST: 0.01
  STEPS: 1
  WD: 0.0
RNG_SEED: 1
SAVE_DIR: ./output
TEST:
  BATCH_SIZE: 40
/root/anaconda3/envs/cotta/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1361: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead
  warnings.warn(
[24/05/14 20:02:59] [cifar100c_vit.py:  147]: model for adaptation: DataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=48, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=48, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=100, bias=True)
  )
)
[24/05/13 20:59:59] [cifar10c_vit.py:  147]: optimizer for adaptation: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.99)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
[24/05/13 20:59:59] [cifar10c_vit.py:   64]: resetting model
[24/05/13 21:05:44] [cifar10c_vit.py:   83]: error % [gaussian_noise5]: 43.78%
[24/05/13 21:05:44] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:11:33] [cifar10c_vit.py:   83]: error % [shot_noise5]: 31.31%
[24/05/13 21:11:33] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:17:14] [cifar10c_vit.py:   83]: error % [impulse_noise5]: 25.17%
[24/05/13 21:17:14] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:23:00] [cifar10c_vit.py:   83]: error % [defocus_blur5]: 16.58%
[24/05/13 21:23:00] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:28:49] [cifar10c_vit.py:   83]: error % [glass_blur5]: 28.17%
[24/05/13 21:28:49] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:34:22] [cifar10c_vit.py:   83]: error % [motion_blur5]: 13.84%
[24/05/13 21:34:22] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:39:58] [cifar10c_vit.py:   83]: error % [zoom_blur5]: 9.79%
[24/05/13 21:39:58] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:45:30] [cifar10c_vit.py:   83]: error % [snow5]: 8.38%
[24/05/13 21:45:30] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:50:56] [cifar10c_vit.py:   83]: error % [frost5]: 7.12%
[24/05/13 21:50:56] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 21:56:37] [cifar10c_vit.py:   83]: error % [fog5]: 10.10%
[24/05/13 21:56:37] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:02:16] [cifar10c_vit.py:   83]: error % [brightness5]: 3.00%
[24/05/13 22:02:16] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:07:49] [cifar10c_vit.py:   83]: error % [contrast5]: 12.97%
[24/05/13 22:07:49] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:13:28] [cifar10c_vit.py:   83]: error % [elastic_transform5]: 12.01%
[24/05/13 22:13:28] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:19:06] [cifar10c_vit.py:   83]: error % [pixelate5]: 16.37%
[24/05/13 22:19:06] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:24:49] [cifar10c_vit.py:   83]: error % [jpeg_compression5]: 13.54%
average: 0.16808666666666666

# hidden dimension = 24
[24/05/13 22:36:30] [cifar10c_vit.py:   64]: resetting model
[24/05/13 22:41:59] [cifar10c_vit.py:   83]: error % [gaussian_noise5]: 65.44%
[24/05/13 22:41:59] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:47:28] [cifar10c_vit.py:   83]: error % [shot_noise5]: 56.30%
[24/05/13 22:47:28] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:52:52] [cifar10c_vit.py:   83]: error % [impulse_noise5]: 32.83%
[24/05/13 22:52:52] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 22:58:16] [cifar10c_vit.py:   83]: error % [defocus_blur5]: 16.21%
[24/05/13 22:58:16] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:03:39] [cifar10c_vit.py:   83]: error % [glass_blur5]: 31.41%
[24/05/13 23:03:39] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:09:03] [cifar10c_vit.py:   83]: error % [motion_blur5]: 13.58%
[24/05/13 23:09:03] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:14:27] [cifar10c_vit.py:   83]: error % [zoom_blur5]: 9.44%
[24/05/13 23:14:27] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:19:56] [cifar10c_vit.py:   83]: error % [snow5]: 8.81%
[24/05/13 23:19:56] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:25:21] [cifar10c_vit.py:   83]: error % [frost5]: 8.38%
[24/05/13 23:25:21] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:30:45] [cifar10c_vit.py:   83]: error % [fog5]: 10.66%
[24/05/13 23:30:45] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:36:09] [cifar10c_vit.py:   83]: error % [brightness5]: 3.09%
[24/05/13 23:36:09] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:41:35] [cifar10c_vit.py:   83]: error % [contrast5]: 16.11%
[24/05/13 23:41:35] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:46:59] [cifar10c_vit.py:   83]: error % [elastic_transform5]: 12.18%
[24/05/13 23:46:59] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:52:24] [cifar10c_vit.py:   83]: error % [pixelate5]: 15.54%
[24/05/13 23:52:24] [cifar10c_vit.py:   68]: not resetting model
[24/05/13 23:57:53] [cifar10c_vit.py:   83]: error % [jpeg_compression5]: 13.37%
average: 0.20890000000000003
[24/05/13 23:57:53] [cifar10c_vit.py:   87]:
 average: 0.20890000000000003

# hidden dimension = 96
[24/05/13 22:41:59] [cifar10c_vit.py:   83]: error % [gaussian_noise5]: 63.42%
[24/05/13 22:41:59] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 12:54:53] [cifar10c_vit.py:   83]: error % [shot_noise5]: 52.43%
[24/05/14 12:54:53] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:02:59] [cifar10c_vit.py:   83]: error % [impulse_noise5]: 36.41%
[24/05/14 13:02:59] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:10:52] [cifar10c_vit.py:   83]: error % [defocus_blur5]: 18.79%
[24/05/14 13:10:52] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:18:58] [cifar10c_vit.py:   83]: error % [glass_blur5]: 31.96%
[24/05/14 13:18:58] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:27:08] [cifar10c_vit.py:   83]: error % [motion_blur5]: 15.41%
[24/05/14 13:27:08] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:35:16] [cifar10c_vit.py:   83]: error % [zoom_blur5]: 11.22%
[24/05/14 13:35:16] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:43:23] [cifar10c_vit.py:   83]: error % [snow5]: 9.77%
[24/05/14 13:43:23] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:51:29] [cifar10c_vit.py:   83]: error % [frost5]: 8.52%
[24/05/14 13:51:29] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:59:41] [cifar10c_vit.py:   83]: error % [fog5]: 12.03%
[24/05/14 13:59:41] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:07:44] [cifar10c_vit.py:   83]: error % [brightness5]: 3.35%
[24/05/14 14:07:44] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:15:54] [cifar10c_vit.py:   83]: error % [contrast5]: 15.59%
[24/05/14 14:15:54] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:24:00] [cifar10c_vit.py:   83]: error % [elastic_transform5]: 12.82%
[24/05/14 14:24:00] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:32:00] [cifar10c_vit.py:   83]: error % [pixelate5]: 17.19%
[24/05/14 14:32:00] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:40:08] [cifar10c_vit.py:   83]: error % [jpeg_compression5]: 14.29%
average: 0.21574666666666664
[24/05/14 14:40:08] [cifar10c_vit.py:   87]:
 average: 0.21574666666666664

# hidden dimension = 192
[24/05/14 12:49:14] [cifar10c_vit.py:   83]: error % [gaussian_noise5]: 59.11%
[24/05/14 12:49:14] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 12:57:54] [cifar10c_vit.py:   83]: error % [shot_noise5]: 47.75%
[24/05/14 12:57:54] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:07:20] [cifar10c_vit.py:   83]: error % [impulse_noise5]: 27.46%
[24/05/14 13:07:20] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:16:45] [cifar10c_vit.py:   83]: error % [defocus_blur5]: 16.08%
[24/05/14 13:16:45] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:26:44] [cifar10c_vit.py:   83]: error % [glass_blur5]: 28.64%
[24/05/14 13:26:44] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:36:39] [cifar10c_vit.py:   83]: error % [motion_blur5]: 13.03%
[24/05/14 13:36:39] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:46:23] [cifar10c_vit.py:   83]: error % [zoom_blur5]: 9.29%
[24/05/14 13:46:23] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 13:56:08] [cifar10c_vit.py:   83]: error % [snow5]: 8.43%
[24/05/14 13:56:08] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:05:59] [cifar10c_vit.py:   83]: error % [frost5]: 7.79%
[24/05/14 14:05:59] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:15:55] [cifar10c_vit.py:   83]: error % [fog5]: 9.86%
[24/05/14 14:15:55] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:25:26] [cifar10c_vit.py:   83]: error % [brightness5]: 3.05%
[24/05/14 14:25:26] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:34:53] [cifar10c_vit.py:   83]: error % [contrast5]: 14.58%
[24/05/14 14:34:53] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:43:25] [cifar10c_vit.py:   83]: error % [elastic_transform5]: 12.31%
[24/05/14 14:43:25] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:50:13] [cifar10c_vit.py:   83]: error % [pixelate5]: 15.57%
[24/05/14 14:50:13] [cifar10c_vit.py:   68]: not resetting model
[24/05/14 14:56:50] [cifar10c_vit.py:   83]: error % [jpeg_compression5]: 13.28%
average: 0.19082
[24/05/14 14:56:50] [cifar10c_vit.py:   87]:
average: 0.19082

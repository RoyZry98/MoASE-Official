[24/05/14 20:00:07] [conf.py:  216]: PyTorch Version: torch=1.10.0, cuda=11.3, cudnn=8200
[24/05/14 20:00:07] [conf.py:  218]: BN:
  EPS: 1e-05
  MOM: 0.1
CKPT_DIR: ./ckpt
CORRUPTION:
  DATASET: cifar100
  NUM_EX: 10000
  SEVERITY: [5]
  TYPE: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blu
r', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
CUDNN:
  BENCHMARK: True
DATA_DIR: /data/home/zhangrongyu/code/cotta/cifar/data
DESC:
LOG_DEST: vit_240514_200007.txt
LOG_TIME: 240514_200007
MODEL:
  ADAPTATION: cotta
  ARCH: Standard_VITB
  EPISODIC: False
OPTIM:
  AP: 0.92
  BETA: 0.9
  DAMPENING: 0.0
  LR: 0.0001
  METHOD: Adam
  MOMENTUM: 0.9
  MT: 0.999
  NESTEROV: True
  RST: 0.01
  STEPS: 1
  WD: 0.0
RNG_SEED: 1
SAVE_DIR: ./output
TEST:
  BATCH_SIZE: 40
/root/anaconda3/envs/cotta/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1361: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead
  warnings.warn(
[24/05/14 21:30:29] [cifar100c_vit.py:  147]: model for adaptation: DataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=100, bias=True)
  )
)
[24/05/14 12:50:08] [cifar100c_vit.py:  149]: optimizer for adaptation: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.99)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
[24/05/14 12:50:08] [cifar100c_vit.py:   60]: 当前进程 ID 是: 122842   
[24/05/14 12:50:09] [cifar100c_vit.py:   66]: resetting model
[24/05/14 12:58:33] [cifar100c_vit.py:   85]: error % [gaussian_noise5]: 42.69%
[24/05/14 12:58:33] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:07:10] [cifar100c_vit.py:   85]: error % [shot_noise5]: 34.24%
[24/05/14 13:07:10] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:15:50] [cifar100c_vit.py:   85]: error % [impulse_noise5]: 20.57%
[24/05/14 13:15:50] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:24:33] [cifar100c_vit.py:   85]: error % [defocus_blur5]: 23.10%
[24/05/14 13:24:33] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:33:18] [cifar100c_vit.py:   85]: error % [glass_blur5]: 38.75%
[24/05/14 13:33:18] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:42:00] [cifar100c_vit.py:   85]: error % [motion_blur5]: 22.20%
[24/05/14 13:42:00] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:50:49] [cifar100c_vit.py:   85]: error % [zoom_blur5]: 17.37%
[24/05/14 13:50:49] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 13:59:28] [cifar100c_vit.py:   85]: error % [snow5]: 18.80%
[24/05/14 13:59:28] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:08:02] [cifar100c_vit.py:   85]: error % [frost5]: 18.07%
[24/05/14 14:08:02] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:16:46] [cifar100c_vit.py:   85]: error % [fog5]: 24.18%
[24/05/14 14:16:46] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:25:15] [cifar100c_vit.py:   85]: error % [brightness5]: 12.74%
[24/05/14 14:25:15] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:33:44] [cifar100c_vit.py:   85]: error % [contrast5]: 24.45%
[24/05/14 14:33:44] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:42:00] [cifar100c_vit.py:   85]: error % [elastic_transform5]: 28.24%
[24/05/14 14:42:00] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:48:52] [cifar100c_vit.py:   85]: error % [pixelate5]: 32.76%
[24/05/14 14:48:52] [cifar100c_vit.py:   70]: not resetting model
[24/05/14 14:55:33] [cifar100c_vit.py:   85]: error % [jpeg_compression5]: 29.07%
average: 0.25815333333333335
[24/05/14 14:55:33] [cifar100c_vit.py:   89]:
 average: 0.25815333333333335
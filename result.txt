
[24/03/19 17:08:17] [cifar10c_vit.py:  285]: model for adaptation: DataParallel(
  (module): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): Sequential(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (adapter): Cotta_Adapter(
          (router): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (router_2): Sequential(
            (0): Linear(in_features=768, out_features=4, bias=True)
            (1): KeepTopK()
            (2): Softmax(dim=-1)
          )
          (adaptmlp): ModuleList(
            (0): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (1): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (2): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
            (3): Adapter(
              (down_proj): Linear(in_features=768, out_features=192, bias=True)
              (non_linear_func): ReLU()
              (up_proj): Linear(in_features=192, out_features=768, bias=True)
              (ourdropout): BiasedDropout()
            )
          )
          (biaseddrop): BiasedDropout()
        )
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
